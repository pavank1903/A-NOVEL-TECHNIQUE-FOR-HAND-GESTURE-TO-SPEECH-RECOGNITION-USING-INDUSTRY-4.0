{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a6e2e-f6a0-4cf6-ab7d-eae888e60718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Class names for hand gestures\n",
    "class_names = ['hello','i love you','man','namaste','no','okay','please','thank you','welcome','yes']\n",
    "\n",
    "# Settings\n",
    "imgsize = 300  # Size of the final square image\n",
    "offset = 20  # Padding around the hand bounding box\n",
    "gesture_gap = 1.0  # Minimum time between gesture inputs (in seconds)\n",
    "pause_duration = 3.0  # Duration to form a sentence after no gestures (in seconds)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to predict hand gesture from an image\n",
    "def predict_hand_gesture(model, img, class_names):\n",
    "    img_height, img_width = 64, 64  # Update to match the model's expected input size\n",
    "    img = cv2.resize(img, (img_height, img_width))  # Resize cropped hand to model input size\n",
    "    img_array = np.expand_dims(img, axis=0) / 255.0  # Normalize and add batch dimension\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_label = class_names[np.argmax(prediction)]\n",
    "    return predicted_label\n",
    "\n",
    "# Function to convert text to speech\n",
    "def speak_text(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Function to detect hand, draw landmarks, crop, and resize the hand region\n",
    "def detect_and_process_hand(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB (required by MediaPipe)\n",
    "    results = hands.process(frame_rgb)  # Process the frame to detect hands\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get the bounding box coordinates\n",
    "            h, w, _ = frame.shape\n",
    "            x_min, x_max = w, 0\n",
    "            y_min, y_max = h, 0\n",
    "\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                x_min, x_max = min(x, x_min), max(x, x_max)\n",
    "                y_min, y_max = min(y, y_min), max(y, y_max)\n",
    "\n",
    "            # Expand the bounding box slightly for a better crop\n",
    "            x_min = max(0, x_min - offset)\n",
    "            y_min = max(0, y_min - offset)\n",
    "            x_max = min(w, x_max + offset)\n",
    "            y_max = min(h, y_max + offset)\n",
    "\n",
    "            # Draw the bounding box on the original frame\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            # Crop the hand region from the frame\n",
    "            cropped_hand = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Resize the cropped hand to a square image while maintaining aspect ratio\n",
    "            img_white = np.ones((imgsize, imgsize, 3), np.uint8) * 255\n",
    "            aspect_ratio = (y_max - y_min) / (x_max - x_min)\n",
    "\n",
    "            if aspect_ratio > 1:\n",
    "                # Height is greater than width\n",
    "                k = imgsize / (y_max - y_min)\n",
    "                w_cal = math.ceil(k * (x_max - x_min))\n",
    "                img_resized = cv2.resize(cropped_hand, (w_cal, imgsize))\n",
    "                w_gap = math.ceil((imgsize - w_cal) / 2)\n",
    "                img_white[:, w_gap:w_gap + w_cal] = img_resized\n",
    "            else:\n",
    "                # Width is greater than height\n",
    "                k = imgsize / (x_max - x_min)\n",
    "                h_cal = math.ceil(k * (y_max - y_min))\n",
    "                img_resized = cv2.resize(cropped_hand, (imgsize, h_cal))\n",
    "                h_gap = math.ceil((imgsize - h_cal) / 2)\n",
    "                img_white[h_gap:h_gap + h_cal, :] = img_resized\n",
    "\n",
    "            return img_white, True  # Return the resized hand image\n",
    "    return frame, False  # Return the original frame if no hand is detected\n",
    "\n",
    "# Function to capture video and process each frame\n",
    "def process_video(model, class_names):\n",
    "    cap = cv2.VideoCapture(0)  # Capture video from webcam\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream.\")\n",
    "        return\n",
    "    \n",
    "    sentence = []\n",
    "    last_gesture_time = 1\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect hand and get the processed hand image\n",
    "        processed_hand, hand_detected = detect_and_process_hand(frame)\n",
    "\n",
    "        if hand_detected and time.time() - last_gesture_time > gesture_gap:\n",
    "            # Predict gesture on the processed hand\n",
    "            predicted_label = predict_hand_gesture(model, processed_hand, class_names)\n",
    "            print(f'Predicted hand gesture: {predicted_label}')\n",
    "            \n",
    "            # Add to sentence list\n",
    "            sentence.append(predicted_label)\n",
    "            last_gesture_time = time.time()  # Update the last gesture time\n",
    "\n",
    "            # Display the frame with predicted label (optional)\n",
    "            cv2.putText(frame, predicted_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # If no new gesture is detected after the pause duration, form the sentence and speak it\n",
    "        if len(sentence) > 0 and time.time() - last_gesture_time > pause_duration:\n",
    "            sentence_str = ' '.join(sentence)\n",
    "            speak_text(sentence_str)\n",
    "            print(f'Sentence spoken: {sentence_str}')\n",
    "            \n",
    "            sentence = []  # Clear the sentence after speaking\n",
    "            start_time = time.time()  # Reset timer\n",
    "\n",
    "        # Display the frame with the landmarks and bounding box\n",
    "        cv2.imshow('Hand Gesture Detection', frame)\n",
    "\n",
    "        # Press 'q' to quit the video capture\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# Load the model before passing it to the function\n",
    "model = tf.keras.models.load_model(\"C:\\\\Users\\\\kumar\\\\OneDrive\\\\Desktop\\\\model\\\\MODEL_3.keras\")\n",
    "\n",
    "# Process the video with the loaded model\n",
    "process_video(model, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
